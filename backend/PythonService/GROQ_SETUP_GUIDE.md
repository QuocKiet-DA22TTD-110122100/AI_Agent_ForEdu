# âš¡ HÆ°á»›ng dáº«n sá»­ dá»¥ng Groq AI (LPU Inference)

## âœ… ÄÃ£ cáº¥u hÃ¬nh

### 1. Groq API Key Ä‘Ã£ thÃªm vÃ o `.env`:
```env
GROQ_API_KEY=your_groq_api_key_here
DEFAULT_AI_MODEL=gemini  # hoáº·c "groq"
```

**Láº¥y API key:** https://console.groq.com/keys

### 2. Code Ä‘Ã£ support:
- âœ… `groq_helper.py` - Client Ä‘á»ƒ gá»i Groq API
- âœ… `main.py` - ÄÃ£ import vÃ  khá»Ÿi táº¡o Groq client
- âœ… Tá»± Ä‘á»™ng chá»n AI model dá»±a vÃ o `DEFAULT_AI_MODEL`

---

## ğŸ”„ CÃ¡ch chuyá»ƒn sang Groq

### Option 1: Chuyá»ƒn máº·c Ä‘á»‹nh sang Groq

Sá»­a file `.env`:
```env
DEFAULT_AI_MODEL=groq
```

Restart AI service:
```powershell
.\restart-ai-service.ps1
```

### Option 2: Cho phÃ©p user chá»n model khi chat

**Frontend** - ThÃªm dropdown chá»n model:
```typescript
<select value={selectedModel} onChange={(e) => setSelectedModel(e.target.value)}>
  <option value="gemini">Gemini 2.5 Flash</option>
  <option value="groq">Groq LPU</option>
</select>
```

**Backend** - API nháº­n model parameter:
```python
@app.post("/chat")
async def chat(request: ChatRequest):
    # request.ai_provider = "gemini" hoáº·c "groq"
    if request.ai_provider == "groq" and groq_client:
        response = groq_client.generate_text(...)
    else:
        # Use Gemini
        model = genai.GenerativeModel(request.model)
        response = model.generate_content(...)
```

---

## ğŸ†š So sÃ¡nh Gemini vs Groq

| Feature | Gemini 2.5 Flash | Groq LPU |
|---------|------------------|-----------|
| **Tá»‘c Ä‘á»™** | âš¡ Cá»±c nhanh | ğŸš€ SiÃªu nhanh (LPU) |
| **Miá»…n phÃ­** | 1500 requests/ngÃ y | 14,400 req/day (free) |
| **Äá»™ thÃ´ng minh** | â­â­â­â­ | â­â­â­â­ |
| **Kiáº¿n thá»©c** | Cáº­p nháº­t | Llama 3.1, Mixtral |
| **Multimodal** | âœ… Vision, Audio | âŒ Text only |
| **GiÃ¡** | FREE | FREE tier generous |

---

## ğŸ“ Groq Models

### Available Models:
```
llama-3.1-70b-versatile  - Best overall (recommended)
llama-3.1-8b-instant     - Fastest inference
mixtral-8x7b-32768       - Long context (32K tokens)
gemma2-9b-it            - Lightweight & fast
```

### Example Request:
```python
from groq_helper import GroqClient

client = GroqClient(api_key="gsk_...")
response = client.generate_text(
    prompt="Explain quantum computing",
    system_prompt="You are a helpful assistant",
    model="llama-3.1-70b-versatile"
)
print(response)
```

---

## ğŸ› ï¸ TODO: TÃ­ch há»£p Groq vÃ o Chat endpoint

Cáº§n update hÃ m `/chat` trong `main.py`:

```python
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, authorization: str = Header(None)):
    """
    Chat endpoint vá»›i support cáº£ Gemini vÃ  Groq
    """
    # Extract AI provider tá»« request hoáº·c dÃ¹ng DEFAULT
    ai_provider = getattr(request, 'ai_provider', DEFAULT_AI_MODEL)
    
    # System prompt
    system_prompt = """ğŸ“ Báº¡n lÃ  AI Learning Assistant..."""
    
    # Generate response dá»±a vÃ o AI provider
    if ai_provider == "groq" and groq_client:
        # Use Groq LPU
        ai_response = groq_client.generate_text(
            prompt=full_prompt,
            system_prompt=system_prompt,
            model="llama-3.1-70b-versatile"
        )
    else:
        # Use Gemini (default)
        gemini_model = genai.GenerativeModel(request.model)
        result = gemini_model.generate_content(full_prompt)
        ai_response = result.text
    
    return ChatResponse(
        response=ai_response,
        model=ai_provider,
        rag_enabled=request.use_rag
    )
```

---

## ğŸ¯ Next Steps

### 1. Test Groq helper trá»±c tiáº¿p:
```powershell
cd backend\PythonService
python groq_helper.py
```

### 2. Update ChatRequest model:
```python
class ChatRequest(BaseModel):
    message: str
    model: str = "gemini-flash-latest"
    ai_provider: str = "gemini"  # NEW: "gemini" or "groq"
    use_rag: bool = True
```

### 3. ThÃªm Groq vÃ o Mode Selector:
```tsx
<button onClick={() => setAiProvider('groq')}>
  âš¡ Groq
</button>
```

### 4. Show AI provider trong chat:
```tsx
<span className="text-xs text-gray-500">
  Powered by {aiProvider === 'groq' ? 'âš¡ Groq LPU' : 'âœ¨ Gemini'}
</span>
```

---

## ğŸ’¡ Use Cases

### Khi nÃ o dÃ¹ng Groq:
- âœ… Cáº§n tá»‘c Ä‘á»™ inference cá»±c nhanh (LPU)
- âœ… Cháº¡y Llama 3.1 70B hoáº·c Mixtral
- âœ… Token limit lá»›n (32K vá»›i Mixtral)
- âœ… Miá»…n phÃ­ 14,400 requests/ngÃ y

### Khi nÃ o dÃ¹ng Gemini:
- âœ… Cáº§n xá»­ lÃ½ áº£nh/video (Vision)
- âœ… Miá»…n phÃ­ 1,500 requests/ngÃ y
- âœ… Tá»‘c Ä‘á»™ cá»±c nhanh
- âœ… Multimodal capabilities

---

## ğŸ” Security Note

âš ï¸ **QUAN TRá»ŒNG:** API keys trong `.env` khÃ´ng Ä‘Æ°á»£c commit lÃªn Git!

Äáº£m báº£o `.env` Ä‘Ã£ cÃ³ trong `.gitignore`:
```
backend/PythonService/.env
```

---

**Grok Ä‘Ã£ sáºµn sÃ ng! Giá» chá»‰ cáº§n tÃ­ch há»£p vÃ o chat endpoint.** ğŸš€
